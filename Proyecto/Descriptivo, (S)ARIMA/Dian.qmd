# Análisis del recaudo de impuestos internos por la DIAN

La DIAN es la entidad encargada de administrar y recaudar los impuestos internos y aduaneros en el país. El recaudo de impuestos internos que realiza la DIAN cada mes se refiere a la suma total de los impuestos nacionales recaudados dentro del territorio colombiano durante ese período mensual. Los impuestos internos son aquellos que se aplican a las actividades económicas y transacciones que ocurren dentro del país, los cuales pueden incluir: IVA, impuesto de renta y complementarios, impuesto de timbre, impuesto de consumo, impuesto a la riqueza, impuesto predial, ICA, entre otros.

Con el proyecto se busca estudiar esta serie de tiempo para ver como es el comportamiento de los impuestos internos de Colombia a lo largo de los años, por ejemplo, encontrar patrones y observar qué tanto han aumentado dichos impuestos.

A continuación se presenta la manera en que se realiza la carga de los datos y un vistazo preliminar de la serie de tiempo en la @fig-serietiempodian .

```{r}
#| label: fig-serietiempodian
#| fig-cap: >
#|    Gráfico de la serie de tiempo de la DIAN.

# Carga de la base de datos
dian<-read_excel("dian.xlsx", range="A7:C313", sheet = "Rec mensual a junio 2023" )
años<-2000:2023
dian<-dplyr::filter(dian,Año %in% años)
colnames(dian)<-c("Año","Mes","Impuestos")
dian$fecha<-as.Date(paste(dian$Año, dian$Mes, "1", sep = "-"), format = "%Y-%B-%d")
dian<-dian[,3:4]

# Gráfico de la serie de tiempo
dian2<-ts(dian$Impuestos,start=c(2000,01),frequency=12)
plot(dian2, main="Serie de tiempo del recaudo mensual interno",
     cex.main=1.3,
     xlab="Tiempo",
     ylab="Recaudo interno",
     cex.lab=0.4)
```

# Análisis descriptivo

```{r, echo=FALSE}
#| warning: false
#| echo: false
library(TSstudio)
library(readxl)
library(dplyr)
library(lubridate)
library(astsa)
library(feasts)
library(fable)
library(timetk)
library(tsibble)
library(zoo)
library(xts)
library(readxl)
library(tidyverse)
library(forecast)
library(plotly)
library(urca)
library(forecast)
library(tseries)
library(lmtest)
library(uroot)
library(fUnitRoots)
library(tsoutliers)
library(readxl)
library(dplyr)
library(lubridate)
library(astsa)
library(feasts)
library(fable)
library(timetk)
library(tsibble)
library(zoo)
library(xts)
library(readxl)
library(tidyverse)
library(forecast)
library(plotly)
setwd("C:\Users\soffy\OneDrive\Escritorio\Universidad\Series\Repo\Proyecto")
```

```{r}
# Base de datos de la DIAN 
dian<-read_excel("dian.xlsx", range="A7:C313", sheet = "Rec mensual a junio 2023" )
años<-2000:2023
dian<-dplyr::filter(dian,Año %in% años)
colnames(dian)<-c("Año","Mes","Impuestos")
dian$fecha<-as.Date(paste(dian$Año, dian$Mes, "1", sep = "-"), format = "%Y-%B-%d")
dian<-dian[,3:4]

# Serie de tiempo de la DIAN 
dian2<-ts(dian$Impuestos,start=c(2000,01),frequency=12)
```

El gráfico de nuestra serie de tiempo viene dado por:

```{r}
plot(dian2, main="Serie de tiempo del recaudo mensual interno",
     cex.main=1.3,
     xlab="Tiempo",
     ylab="Recaudo interno",
     cex.lab=0.4)
```

Según lo observado en la serie de tiempo, preliminarmente se tiene que la serie presenta:

-   **Heteroscedasticidad marginal:** Se puede observar que la varianza de cada instante en la serie no es la misma, a medida que pasa el tiempo esta va aumentando, por lo tanto, surge la necesidad de realizar una transformación Box-cox.
-   **Tendencia:** A simple vista se observa que a medida que pasa el tiempo, la serie oscila al rededor de valores cada vez más grandes, en consecuencia, es necesario estimar la tendencia y posteriormente eliminarla para poder continuar con el análisis de la serie.
-   **Componente estacional:** Se observan algunos patrones que se repiten con cierta periodicidad (posiblemente cada año), lo cual hace que sea necesario estimar posibles comportamientos estacionales.

## Estabilización de la varianza marginal

Teniendo en cuenta lo observado utilizando el gráfico de la serie de tiempo, se puede evidenciar una heteroscedasticidad marginal la cual debemos corregir, para esto utilizaremos la transformación de Box-cox, la cual está dada por la siguiente fórmula. \begin{equation}
    f_{\lambda}(u_{t})= \begin{cases}
        \lambda^{-1}(u^{\lambda}_{t}-1), &  \text{si  $u_{t} \geq 0$, para $\lambda>0$,}\\
        \ln(u_{t}), &\text{ si $u_{t}>0$, para $\lambda=0$}.
    \end{cases}
\end{equation}

Donde el $λ$ apropiado debe ser estimado. Nótese que esta transformación es posible, puesto que no se tienen valores negativos, de este modo se tiene según la @fig-1 que:

```{r}
#| label: fig-1
#| fig-cap: >
#|    Gráfico del valor lambda que maximiza la logverosmilitud.
MASS::boxcox(lm(dian2 ~ 1),seq(-5, 5, length = 50)) ##Notese que no acputra al 1
forecast::BoxCox.lambda(dian2, method ="loglik",
                        lower = -1, upper = 3)#Entrega el valor de lambda (0.1).
```

Con el anterior gráfico y salida podemos observar que el valor que maximiza la log-verosimilitud es 0.1, pero este valor es bastante cercano a 0, por lo tanto, lo aproximaremos a 0 para así poder utilizar la transformación logaritmo, nótese que el intervalo de confianza no captura al 1 en consecuencia puede ser conveniente realizar la transformación. En los gráficos de @fig-2 y @fig-2-1 analizaremos si dicha transformación logra estabilizar la varianza marginal de nuestros datos.

```{r}
#| label: fig-2
#| fig-cap: >
#|    Grafico de la serie con varianza estable utilizando lambda=0.1.
plot(forecast::BoxCox(dian2,lambda=0.1))
```

```{r}
#| label: fig-2-1
#| fig-cap: >
#|    Grafico de la serie con varianza estable utilizando el logaritmo.
par(mar = c(1,1,1,1))
ldian2=log(dian2)
MASS::boxcox(lm(ldian2 ~ 1),seq(-5, 5, length = 50)) #Si captura al 1

```

Nótese que el intervalo de confianza de la transformación Box-cox logra capturar el valor de 1, lo cual nos indica que no es necesario transformar los datos nuevamente o buscar otro valor para $\lambda$, además la transformación ayudo de buena manera a estabilizar la varianza marginal de nuestra serie de tiempo.

En el gráfico @fig-3 y @fig-3-1 mostramos la serie de recaudo de la DIAN con y sin la transformación logaritmo y es posible observar que la escala disminuyey que a su vez se observan algunos cambios considerables en la forma de la serie, lo cual es un buen indicativo de la relevancia de realizar la transformación.

```{r}
#| label: fig-3
#| fig-cap: >
#|    Serie DIAN sin transformar los datos utilizando el logaritmo.
#par(mfrow=c(2,1))
plot(dian2,main="Serie Dian sin Transformar")
```

```{r}
#| label: fig-3-1
#| fig-cap: >
#|    Serie DIAN con transformación de los datos utilizando el logaritmo.
plot(ldian2,main="Series Dian con Transformación BoxCox")
```

Teniendo lo anterior en cuenta, presentaremos un gráfico (@fig-4) de la serie sin tendencia un poco más interactivo e informativo, con el fin de lograr conocer los distintos valores en cada una de las fechas.

```{r}
#| label: fig-4
#| fig-cap: >
#|    Gráfico de la serie DIAN con los datos transformados.

class(ldian2)
dian3<-window(ldian2, start = c(2000,1))
ts_plot(dian3,title="Serie de tiempo del recaudo mensual interno",
        Ytitle="Recaudo interno",
        Xtitle="Tiempo",
        Xgrid=TRUE,
        Ygrid=TRUE)
```

## Tendencia Estimación y eliminación

Luego de estabilizar la varianza marginal de nuestra serie, procederemos a estimar la tendencia y a eliminarla. Para estimar dicha tendencia iniciaremos utilizando una estimación lineal determinística de la tendencia y posteriormente restaremos la tendencia estimada a los datos de nuestra serie. EL ajuste de la recta se presenta en la @fig-5:

```{r}
#| label: fig-5
#| fig-cap: >
#|    Estimación de la tendencia de manera deterministica.
summary(fit<-lm(ldian2~time(ldian2),na.action=NULL))
plot(ldian2,ylab="Recaudo interno") 
abline(fit,col="darkcyan",lwd=2)
```

Preliminarmente, es posible ver que la recta se ajusta de un buen modo a nuestra serie de tiempo, puesto que la tendencia de nuestra serie es monotona creciente, ahora procederemos a observar la serie de tiempo al eliminar la tendencia utilizando este método, esto se observa en la gráfica @fig-6:

```{r}
#| label: fig-6
#| fig-cap: >
#|    Serie DIANsin tendencia.
ElimiTenddian<-ldian2-predict(fit)
plot(ElimiTenddian,main="Serie Dian sin tendencia y con varianza estable",
     cex.main=1.3,
     xlab="Tiempo",
     ylab="Recaudo interno",
     cex.lab=0.4)
```

Es posible observar que nuestra serie de tiempo cambio considerablemente, puesto que la escala de los valores se disminuyó bastante y además los datos oscilan al rededor del 0, pero aun es posible observar una posible tendencia en nuestros datos, por lo tanto procedemos a analizar otras formas de eliminar la tendencia.

```{r}
#| label: fig-7
#| fig-cap: >
#|    Gráfico acf series DIAN sin tendencia.
acf(ElimiTenddian,lag.max=179,main="Acf Serie Dian sin tendencia")
```

Nótese que el gráfico de autocorrelación (@fig-7), nos indica preliminarmente la presencia de posibles componentes estacionales, pero esto lo analizaremos en detalle más adelante.

Pero este no es el único modo de realizar la estimación de la tendencia, también podemos utilizar herramientas no paramétricas, pero como no hemos identificado la componente estacional, estas nos darán una estimación preliminar de la tendencia. La primera de ellas que vamos a utilizar es la descomposición vía filtros de promedios móviles, que se presenta a continuación (@fig-8):

```{r}
#| label: fig-8
#| fig-cap: >
#|    Descomposición via filtros de promedio moviles.
dian_decompo=decompose(ldian2)
plot(dian_decompo)
#dian_decompo$trend
```

Nótese que parece estimar de modo correcto la componente estacional y en la componente residual se observan algunos patrones estacionales, al análizar la tendencia de este modo, podemos ver que aunque parece bastante lineal existen algunos lugares donde se ajusta mejor a ciertos comportamientos de la serie, por lo tanto podria ser viable utilizar esta tecnica para explorar la componente de tendencia.

Ahora procederemos a utilizar la descomposición STL, para hacer un análisis similar(@fig-9).

```{r}
#| label: fig-9
#| fig-cap: >
#|    Descomposición via STL.
library(feasts)
library(fable)
### Gráfico ##
tsibble_dian<-as_tsibble(ldian2)
str(tsibble_dian)
tsibble_dian %>%
  model(
    STL(value ~ trend() +
          season(window = "periodic"),
        robust = TRUE)) %>%
  components() %>%
  autoplot()
```

```{r}
#| label: fig-10
#| fig-cap: >
#|    Desrie DIAN sin tendencia via STL.
### Eliminando la tendencia por STL
modelo_stl <- tsibble_dian %>%
  model(
    STL(value ~ trend() +
          season(window = "periodic"),
        robust = TRUE)
  ) %>%
  components()


ElimiTenddian_STL<-ldian2-modelo_stl$trend
plot(ElimiTenddian_STL,main="Serie Dian sin tendencia y con varianza estable (STL)",
     cex.main=1.3,
     xlab="Tiempo",
     ylab="Recaudo interno",
     cex.lab=0.4)
```

Es posible observar que el gráfico anterior (@fig-10) es bastante similar al obtenido utilizando filtro de promedios móviles (@fig-8) y podemos realizar una interpretación bastante similar, para continuar con nuestro análisis procederemos a continuar utilizando la eliminación de la tendencia via descomposición STL, pues es aquella que presenta un mejor desempeño,también es posible eliminar la tendencia utilizando diferenciación, pero se debe tener en cuenta que la diferenciación puede eliminar la componente estacional que se tenga presente en la serie de tiempo, a continuación se muestra la manera en que se realizó la eliminación de la tendencia utilizando la diferenciación y su comparación con el método STL para eliminar tendencia utilizado en este trabajo (@fig-11).

```{r}
#| label: fig-11
#| fig-cap: >
#|    Serie DIAN sin tendencia via STL vs diferenciación.
par(mar = c(2,2,2,2))
fitdian = lm(ldian2~time(ldian2), na.action=NULL) 
par(mfrow=c(2,1))
plot(ElimiTenddian_STL, type="l", main="Sin tendencia via STL") 
plot(diff(ldian2), type="l", main="Primera Diferencia") #Primera diferencia ordinaria
```

Podemos observar que a simple vista no hay diferencias muy importantes en cada una de las graficas, puesto que ambas se centran en 0 y parecen tener un comportamiento parecido.

En el siguiente gráfico (@fig-12) se compara las funciones de autocorrelación obtenidas para la serie cuando eliminamos la tendencia utilizando descomposición STL y utilizando diferenciación, junto con la función de autocorrelación para la serie de tiempo con tendencia.

```{r}
#| label: fig-12
#| fig-cap: >
#|    Comparacion de los gráficos acf.
par(mar = c(3,2,3,2))
par(mfrow=c(3,1)) # plot ACFs
acf(ldian2, 60, main="ACF Dian con tendencia.")
acf(ElimiTenddian_STL, 60, main="ACF Sin tendencia STL.") 
acf(diff(ldian2), 60, main="ACF Primera Diferencia.")
```

En el anterior gráfico (@fig-12), podemos observar que al eliminar la tendencia, el gráfico ACF baja mucho más rápido en las series de tiempo donde se eliminó la tendencia (por el método STL, como por el método de diferenciación) en comparación con la serie a la cual solo se le ajustó la varianza marginal, pero tanto en la serie diferenciada como en la serie eliminada por STL, se observa la presencia de una alta correlación en rezagos de tamaño 12, lo cual es un indicio de la presencia de una componente estacional.

## Detección de estacionalidad

Luego de estabilizar la varianza marginal y de tratar la tendencia, procedemos a observar si existe presencia de posibles ciclos o ciclos estacionales, para dicha tarea vamos a emplear múltiples métodos descriptivos que nos permiten obtener información sobre esta componente. Iniciaremos por observar el gráfico de retardos, el cual viene dado a continuación:

```{r}
#| label: fig-13
#| fig-cap: >
#|    Gráfico de retardos.
ts_info(ElimiTenddian_STL)
par(mar = c(3,2,3,2))
astsa::lag1.plot(ElimiTenddian_STL, 12,corr=F)
```

```{r}
#| label: fig-13-1
#| fig-cap: >
#|    Gráfico de retardos.

ts_lags(ElimiTenddian_STL,lags=1:12)
```

El gráfico de retardos dado en @fig-13 y @fig-13-1, nos indican de manera descriptiva la posible relación existente entre un tiempo y algunos de sus retardos, para este caso en particular se toman 12 retardos (esto teniendo en cuenta la frecuencia mensual de la serie de tiempo), en este caso es posible observar que existe una clara relación lineal y directa con el rezago 12, los demás rezagos no parecen ser del todo significativos. Nótese que los dos gráficos anteriores nos dan una información bastante similar.

Ahora observemos el gráfico de sub series (@fig-14), el cual toma los valores por cada mes de cada uno de los años dentro de la serie, como sabemos, se busca observar si en el histórico encontramos diferentes valores medios, mes tras mes, de este modo tenemos lo siguiente:

```{r}
#| label: fig-14
#| fig-cap: >
#|    Gráfico de subseries.
require(feasts)
dian2_tsbl_notend=as_tsibble(ElimiTenddian_STL)
dian2_tsbl_notend%>%gg_subseries(value)
```

Al analizar el gráfico de sub series (@fig-14) es posible observar que la media en cada uno de los meses es distinta y no oscilan al rededor de un mismo valor, tomando su valor máximo en los meses de enero, disminuyendo luego en los meses de febrero y marzo, luego vuelve a aumentar y a mantenerse estable durante los meses de abril, mayo y junio. Esto es un claro indicio de la presencia de una componente cíclica estacional o cíclica. A continuación se presentan algunas otras gráficas descriptivas para observar la presencia de un ciclo estaciona.

```{r}
#| label: fig-15
#| fig-cap: >
#|    Promedio mensual.
dian2sint_df <- data.frame(year = floor(time(ElimiTenddian_STL)), month = cycle(ElimiTenddian_STL),ElimiTenddian_STL = as.numeric(ElimiTenddian_STL))
dian2sint_df$month <- factor(month.abb[dian2sint_df$month], levels = month.abb)
dian2sint_summary <- dian2sint_df %>%group_by(month) %>%summarise(mean= mean(ElimiTenddian_STL),sd = sd(ElimiTenddian_STL))
dian2sint_summary
plot_ly (data = dian2sint_summary, x = ~ month, y = ~ mean, type = "bar", name   = "Mean") %>%
  layout (title = "dian2sint - Monthly Average", yaxis =list(title = "Mean",   range = c(min(dian2sint_summary$mean), max(dian2sint_summary$mean))))
```

En el anterior gráfico (@fig-15) se observa el valor medio tomado por cada uno de los meses, es posible observar que tiene un comportamiento parecido al gráfico de sub series y de manera análoga nos muestra que existe una componente estacional.

A continuación se muestran los mapas de calor para la serie DIAN con varianza marginal estable y tendencia eliminada por el método de descomposición STL y el metodo de diferenciacio STL.

```{r}
#| label: fig-16
#| fig-cap: >
#|    Graficos de calor serie DIAN sin tendencia por STLn.
TSstudio::ts_heatmap(ElimiTenddian_STL,title = "Mapa de Calor - Impuestos Dian sin tendencia (STL)")
```

```{r}
#| label: fig-16-1
#| fig-cap: >
#|    Graficos de calor serie DIAN sin tendencia por diferenciación.
TSstudio::ts_heatmap(diff(ldian2),title = "Mapa de Calor - Impuestos Dian sin tendencia (dif)")
```

Ambos mapas (@fig-16 y @fig-16-1) nos dan una información similar, en los cuales es posible observar que en los meses de noviembre, enero, junio, mayo y abril se tienen una mayor cantidad de recaudo de impuestos, mientras que en diciembre, octubre, agosto, marzo y febrero tienen un menor valor de recaudo de impuestos año tras año, pero se observa un valor grande en mayo del 2007, nuevamente este gráfico nos ayuda a comprender la existencia de un ciclo estacional que posiblemente tenga un periodo de 12 meses. Es importante resaltar que el tener un valor medio alto en el mes de enero puede deberse al pago del impuesto predial en los primeros meses del año.

Cuando hablamos de una componente estacional dentro de nuestra serie de tiempo, también necesitamos hablar de su periodo y de su frecuencia, para esto utilizaremos el periodograma.

```{r}
#| label: fig-17
#| fig-cap: >
#|    Peridograma serie DIAN.
spectrum(as.numeric(ElimiTenddian_STL),log='no')
abline(v=0.5, lty=2,col="red")
```

```{r}
#| label: fig-17-1
#| fig-cap: >
#|    Peridograma serie DIAN.
spectrum(as.numeric(ElimiTenddian_STL),log='no',span=5)
```

```{r}
#| label: fig-17-2
#| fig-cap: >
#|    Peridograma serie DIAN.
spectrum(as.numeric(ElimiTenddian_STL),log='no',span=c(5,5))
```

```{r}
#| label: fig-17-3
#| fig-cap: >
#|    Peridograma serie DIAN.
spectrum(as.numeric(ElimiTenddian_STL),log='no',span=c(2,2))
```

Al observar los gráficos anteriores (@fig-17,@fig-17-1,@fig-17-2,@fig-17-3), es posible observar que se tienen diferentes valores de suavizamiento para nuestro periodograma, pues aunque en este caso no es difícil observar los puntos donde se tiene un pico, el suavizamiento puede ayudarnos a observar de modo más simple los picos que son verdaderamente significativos.

```{r}
#| label: fig-18
#| fig-cap: >
#|    Periodograma serie DIAN.
Periodgramadldian2_sintendencia=spectrum(as.numeric(ElimiTenddian_STL),log='no')
ubicacionlogdian=which.max(Periodgramadldian2_sintendencia$spec)
sprintf("El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s",Periodgramadldian2_sintendencia$freq[ubicacionlogdian])

sprintf("El periodo correspondiente es aproximadamente: %s",1/Periodgramadldian2_sintendencia$freq[ubicacionlogdian])
```

Nótese que según la salida obtenida, la frecuencia máxima se alcanza en 0.5 (es decir, en 6/12=0.5) y se obtuvo que el periodo es 2, esto quiere decir que el ciclo se repite cada dos meses, pero se tiene que 12 es un múltiplo de 2, por lo tanto, en realidad se tiene que el periodo de la componente estacional seria de tipo anual (12 meses)

## Desestacionalizar o eliminación de la componente estacional

Con el fin de realizar un correcto análisis de la serie de tiempo en la cual no se tenga la interferencia de la estacionalidad, sino observar los verdaderos cambios de la serie de tiempo, procedemos a realizar la estimación de la estacionalidad utilizando componentes de Fourier. En este caso se van a utilizar 3 componentes de fourier teniendo en cuenta lo observado en el análisis realizado en el notebook de phyton.
```{r}
# Frecuencia angular w=2*pi/s
frec_ang=(2*pi/12)

dian_copia=ElimiTenddian_STL

#Fourier k=1 
dian_copia$sin = sin(c(1:282)*(1*frec_ang))
dian_copia$cos = cos(c(1:282)*(1*frec_ang))

#Fourier k=2 
dian_copia$sin2 = sin(c(1:282)*(2*frec_ang))
dian_copia$cos2 = cos(c(1:282)*(2*frec_ang))

#Fourier k=3 
dian_copia$sin3 = sin(c(1:282)*(3*frec_ang))
dian_copia$cos3 = cos(c(1:282)*(3*frec_ang))
#Fourier k=4
dian_copia$sin4 = sin(c(1:282)*(4*frec_ang))
dian_copia$cos4 = cos(c(1:282)*(4*frec_ang))

linmodel_ciclo_dian<-lm(ElimiTenddian_STL~sin+cos+sin2+cos2
                        +sin3+cos3+sin4+cos4,data=dian_copia)
summary(linmodel_ciclo_dian)

results_ciclo_dian=linmodel_ciclo_dian$fitted.values
results_ciclo_dian<-as.data.frame(results_ciclo_dian)
results_ciclo_ts<-ts(results_ciclo_dian,start=c(2000,01),frequency=12)
```
```{r}
# Series original
plot(ElimiTenddian_STL, main="Serie de tiempo de recaudo de impuestos DIAN",
     cex.main=1.3,
     xlab="Tiempo ",
     ylab="Recaudo",
     cex.lab=0.4)

# Estimación de la estacionalidad
lines(results_ciclo_ts,col="red")

# Serie sin estacionalidad
plot(ElimiTenddian_STL-results_ciclo_ts)
```
```{r}
acf(as.numeric(ElimiTenddian_STL-results_ciclo_ts))
```
## Eliminación de la estacionalidad utilizando variables dummy 
```{r}
## Prueba de crear las variables dummy 
library(stats)
library(forecast)
library(ggplot2)

# Carga tus datos o crea una serie de tiempo similar
# Puedes cargar tus datos desde un archivo o crearlos manualmente
# Aquí se asume que tienes una serie de tiempo en un objeto llamado 'dian_detrend_STL'

# Crea un objeto de variable dummy estacional
dummy <- seasonaldummy(ElimiTenddian_STL)

# Ajusta un modelo de regresión lineal
modelo <- lm(ElimiTenddian_STL ~ dummy)

# Obtiene un resumen del modelo
summary(modelo)

# Realiza predicciones con el modelo
nuevas_obs <- length(ElimiTenddian_STL)  # Número de observaciones en la serie de tiempo
predicciones <- predict(modelo, newdata = data.frame(dummy = dummy))

# Crea un nuevo objeto de serie de tiempo con las predicciones
dian_pred <- ts(predicciones, start = start(ElimiTenddian_STL), frequency = frequency(ElimiTenddian_STL))

# Grafica los resultados
plot(ElimiTenddian_STL, col = "blue", type = "l", xlab = "Fecha", ylab = "Valor Original")
lines(dian_pred, col = "red")
legend("topleft", legend = c("Impuestos", "Predicciones"), col = c("blue", "red"))
```
```{r}
plot(ElimiTenddian_STL-dian_pred, col = "blue", type = "l", xlab = "Fecha", ylab = "Impuestos",main="Series sin tendencia (STL) y sin estacionalidad (dummy)")
```


```{r}
acf(as.numeric(ElimiTenddian_STL-dian_pred))
```
# Suavizamiento exponencial.
En el modelo por medio de suavizamiento exponencial también se considera una descomposición de la serie de forma aditiva. Las componentes de tenendecia y la estacionalidad se estiman por medio de una estadística EWMA (promedio movil ponderado exponencialmente), dándole más peso a las observaciones más cercanas en cada tiempo.

Además, para este caso, se descompone la componente de tendencia en nivel y pendiente, y se estima un parámetro de la componente estacional por mes, como lo indica el periodo hallado de 12.  Las estimaciones se hallan de la siguiente manera:


$$\begin{align*}
\text{Componente de nivel: } & a_t=α(x_t-S_{t-p})+(1−α)(a_{t−1}+b_{t−1})\\
\text{Componente de pendiente: } & b_t=β(a_t−a_{t−1})+(1−β)b_{t−1} \\
\text{Componente estacional: } & S_t=\gamma(x_t−a_t)+(1−γ)S_{t−p} \\
\end{align*}$$

Preparando los datos y tambien deividiendo el conjunto de datos para posteriormente Observar cual es el mejor modelo segun su capacidad predictiva.

```{r, echo=FALSE}
#| warning: false
#| echo: false
library(dplyr)
library(parsnip)
library(rsample)
library(timetk)
library(modeltime)
library(tsibble)
library(tidymodels)
```
## Division de los datos en entrenamiento y prueba.
En primer lugar se realizará la división de los datos en el conjunto de entrenamiento y de prueba.
```{r}
lserie=length(ldian2)
ntrain=trunc(length(ldian2)*0.80) ##% del datos en el conjunto de entrenamiento es del 85%.
ntrain
time(ldian2)
time(ldian2)[ntrain]###Me entrega la ultima fecha de la posición ntrain
train=window(ldian2,end=time(ldian2)[ntrain])
test=window(ldian2,start=time(ldian2)[ntrain]+1/12)##1/12 porque es la fracción que corresponde a un mes
length(train)
ntest=length(test)
ntest ##Me define el valor de origins, o de ventanas de rolling.
lserie ### Comparar los valores
```
Luego de dividir los datos procedemos a crear algunos vectores necesarios para realizar el rolling y posteriormente realizaremos el modelo sobre los datos de entrenamiento.
## Modelamiento SE.
```{r}
h=1
fchstepahe=matrix(0,nrow=ntest,ncol=h) ##Crea una Columna para los h-pasos adelante
### verval contiene los verdaderos valores de la serie en el conjunto de prueba con los que se compararán los pronósticos.
verval=cbind(test[1:ntest])
for(j in 2:h){
  verval=cbind(verval,c(test[j:ntest],rep(NA,j-1)))
}

verval=cbind(test[1:ntest],c(test[2:ntest],NA),c(test[3:ntest],NA,NA))
####Ajuste del modelo con los datos de entrenamiento
HWAP_train=stats::HoltWinters(train,seasonal="additive")
HWAP_train$alpha
HWAP_train$beta
HWAP_train$gamma
```
### Análisis de reisulaes 
El análisis de los residuales se hace sobre el conjunto de entrenamiento del siguiente modo.
```{r}
library(stats)
# Obtener residuales del modelo Holt-Winters
residuales_hw <- residuals(HWAP_train)
# Visualizar los residuales en un gráfico
plot(residuales_hw, type = "l", col = "blue", ylab = "Residuales", main = "Residuales del modelo Holt-Winters")
acf(as.numeric(residuales_hw))
pacf(as.numeric(residuales_hw))
#Test de Ljung-Box
# Longitud de los residuos dividida por 4
longitud_dividida <- length(residuales_hw) / 4
cat("Longitud de los residuos dividida por 4:", longitud_dividida, "\n")
# Raíz cuadrada de la longitud de los residuos
raiz_cuadrada <- sqrt(length(residuales_hw))
cat("Raíz cuadrada de la longitud de los residuos:", raiz_cuadrada, "\n")
# Test de Ljung-Box para autocorrelación
library(stats)
# Lags a considerar (24 en este caso)
lags <- 24
# Realizar el test de Ljung-Box
ljung_box_test <- Box.test(residuales_hw, lag = lags, type = "Ljung-Box", fitdf = 0)
cat("Estadística de Ljung-Box:", ljung_box_test$statistic, "\n")
cat("P-valor:", ljung_box_test$p.value, "\n")
```
En este caso es posible observar que al revisar las graficas del ACf y PACF no se evidencian que se tengan cosas por explicar, además la prueba de Ljung-Box, nos da un p valor de 0.2530558 el cual es más grande que un valor $\alpha=0.05$, es decir que existe evidencia estadisticamente significativa para rechazar la hipotesis de correlación, es decir que los residuales no parecen estar correlacionados.
Luego de esto procederemos a realizar Rolling para conocer la capacidad predictiva del modelo.
### Rolling SE

```{r}
##Rolling
for(i in 1:(ntest))
{
  x=window(ldian2,end=time(ldian2)[ntrain]+(i-1)/12)
  print(length(x))
  refit=stats::HoltWinters(x,alpha=0.09446078,beta=0.06537417 ,gamma=0.4091491 ,seasonal="additive")
    fchstepahe[i,]=as.numeric(forecast::forecast(refit,h=h)$mean)
}
fchstepahe
errores_pred=exp(verval[,1]) -exp(fchstepahe) ##Observación: debo devolver los pronósticos y los verdaderos valores a la escala original si es necesario.
ECM=apply(errores_pred^2,MARGIN = 2,mean,na.rm=TRUE) ##Acá se computa la medida de precisión del pronóstico(en este caso ECM).
RECM=sqrt(ECM) ##Se le saca raíz 
RECM ##se lee: Primera fila RECM 1-paso adelante y así sucesivamente.

```
De este modo se obtuvo un valor ECM de 4.595703e+12 (Billones) y un valor RECM de 2143759 (Millones)
### Predicción usando el modelo SE.
```{r}
verval_ts<-ts(exp(verval[,1]),start=time(ldian2)[ntrain]+1/12,frequency=12)
fchstepahe_ts<-ts(exp(fchstepahe),start=time(ldian2)[ntrain]+1/12,frequency=12)

plot(verval_ts, col = "blue", ylab = "Impuestos", xlab = "Tiempo")
lines(fchstepahe_ts, col = "red")
legend("topright", legend = c("Reales", "Predicciones"), col = c("blue", "red"), lty = 1)
```

# Modelamiento ARMA
En el siguiente aparatado se busca ajustar los modelos concernientes a la familia ARMA, esto quiere decir que se va a intentar realizar un modelamiento para un modelo AR, MA y ARMA, posteriormente se observara cual de los modelos ajustados es mejor en terminos de su capacidad predictiva comparando sus ECM.

Teniendo en cuenta que para realizar el modelamiento utilizando ARMA es necesario que la serie de tiempo sea estacionaria, se utilizara la serie sin tendencia por medio de descomposición STL y se realizará el modelamiento simultaneamente eliminando la estacionalidad por medio de variables dummy y de componentes de Fourier.

## Modelamiento ARMA con componentes de fourier.
En primer lugar iniciamos utilizando la serie sin tendencia (via STL), con varianza estable y con estacionalidad eliminada (via componentes de fourier), se observaran su gráficos acf y su gráfico pacf para encontrar los posibles ordens p y q, para realizar los modelamientos.
```{r}
ElimiTenddian_STL_fourier<-ElimiTenddian_STL-results_ciclo_ts

acf(as.numeric(ElimiTenddian_STL_fourier),main="Series estacionaria (STL-Fourier)",lag.max = length(dian2)/4)
acf(as.numeric(ElimiTenddian_STL_fourier),main="Series estacionaria (STL-Fourier)",lag.max = length(dian2)/4,ci.type='ma')## Para el MA
pacf(as.numeric(ElimiTenddian_STL_fourier),main="Series estacionaria (STL-Fourier)",lag.max = length(dian2)/4)
```

```{r}
require(feasts)
dian2_tsbl_notend=as_tsibble(ElimiTenddian_STL_fourier)
dian2_tsbl_notend%>%gg_subseries(value)
```
Este gráfico nos indica que la series sin tendencia via STL y sin estacionalidad via componentes de fourier no hace que nuestra series de tiempo sea estacionaria, esto puede deberse a que las componentes de fourier no ayudaron a estimar de modo correcto la estacionalidad de este proceso.
## Modelamiento ARMA con dummy.
Teniendo en cuenta que el metodo que mejor estimó la estacionalidad fue el metodo de variables dummy, se procederá a realizar el modelado de 3 modelos, AR,MA y ARMA, luego procederemos a compararlos en terminos de su capacidad predictiva (ECM).
## División de la base de datos en entrenamiento, validación y prueba.
```{r}
train_weight2 <- 0.8
split2 <- as.integer(length(ElimiTenddian_STL) * train_weight2)
#window(ldian2,end=time(ldian2)[ntrain])
df_train2 <- window(ElimiTenddian_STL, end = time(ElimiTenddian_STL)[split2])#80%
df_test2 <- window(ElimiTenddian_STL, start = time(ElimiTenddian_STL)[split2] + 1/12,)#20%
```
## Modelo AR
En primer lugar es necesario realizar la estimación de las variables dummy sobre el conjunto de prueba lo cual se va a hacer a continuación.
### Variables dummy 
```{r}
## Prueba de crear las variables dummy 
library(stats)
library(forecast)
library(ggplot2)

# Carga tus datos o crea una serie de tiempo similar
# Puedes cargar tus datos desde un archivo o crearlos manualmente
# Aquí se asume que tienes una serie de tiempo en un objeto llamado 'dian_detrend_STL'
# Crea un objeto de variable dummy estacional
dummy <- seasonaldummy(df_train2)
# Ajusta un modelo de regresión lineal
modelo <- lm(df_train2 ~ dummy)
# Obtiene un resumen del modelo
summary(modelo)
# Realiza predicciones con el modelo
nuevas_obs <- length(df_train2)  # Número de observaciones en la serie de tiempo
predicciones <- predict(modelo, newdata = data.frame(dummy = dummy))
# Crea un nuevo objeto de serie de tiempo con las predicciones
dian_pred_train <- ts(predicciones, start = start(df_train2), frequency = frequency(df_train2))

# Grafica los resultados
plot(df_train2, col = "blue", type = "l", xlab = "Fecha", ylab = "Valor Original")
lines(dian_pred_train, col = "red")
legend("topleft", legend = c("Impuestos", "Predicciones"), col = c("blue", "red"))
```
```{r}
#Eliminando la estacionalidad.
ElimiTenddian_STL_dummy<-df_train2-dian_pred_train
plot(ElimiTenddian_STL_dummy,main="Datos estacionarios")
acf(as.numeric(ElimiTenddian_STL_dummy),main="ACF datos estacionarios",lag.max = length(ElimiTenddian_STL_dummy)/4)
acf(as.numeric(ElimiTenddian_STL_dummy),main="ACF datos estacionarios",lag.max = length(ElimiTenddian_STL_dummy)/4,ci.type='ma')
pacf(as.numeric(ElimiTenddian_STL_dummy),main="PACF datos estacionarios",lag.max = length(ElimiTenddian_STL_dummy)/4)
require(feasts)
dian2_tsbl_notend=as_tsibble(ElimiTenddian_STL_dummy)
dian2_tsbl_notend%>%gg_subseries(value)
```

### Ajustando el modelo AR(12)
Al observar los graficos del acf y el pacf es posible observar que el ACF desciende lentamente hacia 0, mientras que en el PACF se observa que el rezago 12 es significativamente diferente de 0 y rezagos más grandes no parecen ser significativamente distintos de 0, por esta razon nos decantamos por construir un modelo de tipo AR(12)

```{r}
library(lmtest)
ARPURO=Arima(ElimiTenddian_STL_dummy,order=c(12,0,0),include.mean = TRUE)
coeftest(ARPURO)
##Refinamiento del modelo 
ARPURO_ref=Arima(ElimiTenddian_STL_dummy,order=c(12,0,0),include.mean = TRUE,
                 fixed=c(NA,0,0,0,0,0,0,0,0,0,0,NA,0))
### Modelo más parsimonioso que el anterior.
ARPURO_ref_prueba=Arima(ElimiTenddian_STL_dummy,order=c(1,0,0),seasonal = c(1,0,0),include.mean = F)
coeftest(ARPURO_ref)
coeftest(ARPURO_ref_prueba)
```
Al realizar el refinamiento es posible observar que unicamente se encontraron como significativos los parametros asociados con los retardos 1 y 12. Posteriormente se va a realizar el análisis de residuales del modelo AR(12).
### Verificación de supusetos modelo AR
```{r }
# Análisis de residuales
residuales=ARPURO_ref_prueba$residuals
plot(residuales)
acf(residuales)
acf(residuales^2)
pacf(residuales)
```
Es posible observar con los graficos para el ACf y el PACF que no parece que quede algo por explicar dentro de los residuales de este modelo ajustado, lo cual es un buen indicio.
```{R}
#Test de normalidad
tseries::jarque.bera.test(residuales)
#Test de autocorrelación
length(residuales)/4
sqrt(length(residuales))
Box.test(residuales, lag =17 , type = "Ljung-Box", fitdf = 2)#No puedo Rechazar la hipótesis de no autocorrelación!
```
En este caso se tiene que el test de normalidad dado por el test de jarque bera, tiene un p valor de 2.2e-16, el cual es menor que un valor alpha del 0.05, por lo tanto existe suficiente evidencia estadistica para rechazar la hipotesis de normalidad.
En el caso de la prueba de autocorrelación se obtiene un p valor de 0.8409, lo cual indica que  no hay suficiente evidencia para afirmar que hay autocorrelación significativa en los residuos.
```{R}
###Estad?sticas CUSUM
res=residuales
cum=cumsum(res)/sd(res)
N=length(res)
cumq=cumsum(res^2)/sum(res^2)
Af=0.948 ###Cuantil del 95% para la estad?stica cusum
co=0.10997####Valor del cuantil aproximado para cusumsq para n/2
LS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)
LI=-LS
LQS=co+(1:length(res))/N
LQI=-co+(1:length(res))/N
plot(cum,type="l",ylim=c(min(LI),max(LS)),xlab="t",ylab="",main="CUSUM")
lines(LS,type="S",col="red")
lines(LI,type="S",col="red")
#CUSUMSQ
plot(cumq,type="l",xlab="t",ylab="",main="CUSUMSQ")                      
lines(LQS,type="S",col="red")                                                                           
lines(LQI,type="S",col="red")
```
Este gráfico mide la variabilidad de los residuales, en este caso se tiene que se sale un poco la variabilidad de las bandas de confianza, pero esto no ocurre por un largo periodo de tiempo. 

### Rolling AR()
```{r,warning=FALSE}
# rolling 
#ElimiTenddian_STL-dian_pred
h=1
fcmat=matrix(0,nrow=ntest,ncol=h)
for(i in 1:ntest){
  x=window((ElimiTenddian_STL-dian_pred),end=time(ElimiTenddian_STL-dian_pred)[ntrain]+(i-1)/12)
  #print(length(x))
  refit=Arima(x,model=ARPURO_ref_prueba)
  fcmat[i,]=as.numeric(forecast::forecast(refit,h=h)$mean)
}

# para volver a la escala original
estacionalidad<-as.vector(dian_pred)
tendencia<-as.vector(modelo_stl$trend)
fchstepahe<-(fcmat+estacionalidad[226:282])+tendencia[226:282] # primero sumamos la estacionalidad y luego la tendencia

errores_pred=exp(verval[,1]) -exp(fchstepahe) ##Observación: debo devolver los pronósticos y los verdaderos valores a la escala original si es necesario.
ECM=apply(errores_pred^2,MARGIN = 2,mean,na.rm=TRUE) ##Acá se computa la medida de precisión del pronóstico(en este caso ECM).
RECM=sqrt(ECM) ##Se le saca raíz 
RECM # 1308387
```
Con el Rolling sobre el modelo AR(12) se obutov un valor ECM de 1.711875e+12 (Billones) y un valor RECM de 1308387 (Millones)
### Predicción usando el modelo AR(12)
```{r}
verval_ts<-ts(exp(verval[,1]),start=time(ldian2)[ntrain]+1/12,frequency=12)
fchstepahe_ts<-ts(exp(fchstepahe),start=time(ldian2)[ntrain]+1/12,frequency=12)

plot(verval_ts, col = "blue", ylab = "Impuestos", xlab = "Tiempo")
lines(fchstepahe_ts, col = "red")
legend("topright", legend = c("Reales", "Predicciones"), col = c("blue", "red"), lty = 1)
```

# Prueba de raices unitarias.
Antes de proceder con algun modelo ARIMA o SARIMA, realizaremos la prueba de raices unitarias de Dickey Fuller.
```{r}
##Aplicando la prueba de Dickey Fuller sobre la serie con varianza  estabilizada
pacf(ldian2)## SErie dian en escala logaritmica (Boxcox)
ar(ldian2)

resultadodf_1<-adf.test(ldian2,k=15)
resultadodf_1
summary(resultadodf_1)
```
En este caso se obtuvo un p-valor de 0.4965, el cual es mayor que un valor $\alpha=0.05$, lo que nos indica no se tiene evidencia estadistica suficiente para rechazar la hipótesis nula de que la serie temporal tiene una raíz unitaria.
Ahora procederemos a realizar la diferenciación ordinaria y luego se analizará si es necesario realizar una nueva diferenciación.
```{r}
#Diferenciando la serie de tiempo 
dldian2=diff(ldian2)
plot(dldian2)
```

```{r}
pacf(as.numeric(dldian2),lag.max=length(dldian2)/4)
acf(as.numeric(dldian2),lag.max = length(dldian2)/4)
ar(dldian2)
adf.test(dldian2,k=13)
summary(ur.df(dldian2,type="trend",lags = 13))
monthplot(dldian2)
spectrum(dldian2)
```
Al utilizar ambos paquetes en este caso, se tiene que en ambas situaciones se obtiene un p-valor pequeño, el cual es menor que $\alpha=0.05$, lo que quiere decir que hay evidencia estadística para afirmar que la serie temporal no tiene una raíz unitaria, por lo tanto no sería necesario realizar más diferenciaciones ordinarias.

# Modelo ARIMA con variables dummy 
## Ajustando el modelo ARIMA
En primer lugar es necesario diviir los datos en conjunto de entrenamiento y de prueba.
```{r}
## Divison de los datos Utilizando los datos con varianza estable por boxcox.

train_weight2 <- 0.8
split2 <- as.integer(length(dldian2) * train_weight2)
#window(ldian2,end=time(ldian2)[ntrain])
df_train3 <- window(ldian2, end = time(ldian2)[split2])#80%
df_test3 <- window(ldian2, start = time(ldian2)[split2] + 1/12)#20%

```
Por lo observado en los graficos de acf, tiene snetido realizar un ajuste de un modelo ARIMA(12,1,0), puesto que el grafico del ACF baja lentamente y luego del rezago 12, la autocorrelación parcial se hace significativamente igual a 0 (observando el grafico PACF).

```{r,warning=FALSE}
dummy_ARIMA<-forecast::seasonaldummy(df_train3)
###Variables Dummy
ARIMA<-Arima(df_train3,order=c(12,1,0),include.mean = TRUE,xreg = dummy_ARIMA)
coeftest(ARIMA)
## Refinamiento 
ARIMA_ref<-Arima(df_train3,order=c(12,1,0),include.mean = TRUE,xreg = dummy_ARIMA,fixed=c(NA,NA,NA,NA,0,0,0,0,0,0,0,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,0,NA))
coeftest(ARIMA_ref)
```
Ahora procedemos a observar si se cumplen los supuestos de este modelo.
## Verificación de los supuestos.
```{r}
## Validación de los supuestos.
# Análisis de residuales
residuales <-ARIMA_ref$residuals 

plot(residuales)
acf(as.numeric(residuales))
acf(as.numeric(residuales^2))
pacf(as.numeric(residuales))
```

```{R}
#Test de normalidad
tseries::jarque.bera.test(residuales)
#Test de autocorrelación
length(residuales)/4
sqrt(length(residuales))
Box.test(residuales, lag =17 , type = "Ljung-Box", fitdf = 2)#No puedo Rechazar la hipótesis de no autocorrelación!
```
En este caso se tiene que se rechaza el supuesto de normalidad ya que 2.2e-16 (p-value) es menor que un valor alpha de 0.05,en este caso el p-valor de la prueba Box-Ljung es 0.002655, que es menor que 0.05, por lo tanto hay suficiente evidencia para no rechazar que hay autocorrelación significativa en los residuos.
```{R}
###Estad?sticas CUSUM
res=residuales
cum=cumsum(res)/sd(res)
N=length(res)
cumq=cumsum(res^2)/sum(res^2)
Af=0.948 ###Cuantil del 95% para la estad?stica cusum
co=0.10997####Valor del cuantil aproximado para cusumsq para n/2
LS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)
LI=-LS
LQS=co+(1:length(res))/N
LQI=-co+(1:length(res))/N
plot(cum,type="l",ylim=c(min(LI),max(LS)),xlab="t",ylab="",main="CUSUM")
lines(LS,type="S",col="red")
lines(LI,type="S",col="red")
#CUSUMSQ
plot(cumq,type="l",xlab="t",ylab="",main="CUSUMSQ")                      
lines(LQS,type="S",col="red")                                                                           
lines(LQI,type="S",col="red")
```
# Modelos SARIMA
## División de los datos 
```{r}
train_weight2 <- 0.8
split2 <- as.integer(length(ldian2) * train_weight2)
#window(ldian2,end=time(ldian2)[ntrain])
df_train4 <- window(ldian2, end = time(ldian2)[split2])#80%
df_test4 <- window(ldian2, start = time(ldian2)[split2] + 1/12)#20%
```
Teniendo en cuenta la prueba de raices unitarias realizada anteriormente se conoce que hay presencia de una raíz unitaria, por lo cual se deberia realizar una diferenciación ordinaria, por otro lado se sabe de la sección del análisis descriptivo que se tiene estacionalidad de periodo 12, por dicha razón procederemos a realizar diferenciacion estacional, posteriormente observaremos los posibles ordenes para el modelamiento SARIMA.
```{r}
##Diferencia estacional.
dldian2<-diff(df_train4)
monthplot(dldian2)
acf(as.numeric(dldian2),lag.max = length(dian2)/4)
spectrum(dldian2)
nsdiffs(dldian2)
nsdiffs(ldian2)
```
La función "nsdiffs", no indica la cantidad de diferencias estacionales que se deben hacer, en este caso se nos indica que es 1, a su ves se conovia gracias a la sección descriptiva que se tenia presencia de una componente estacional de periodo $s=12$.
```{r}
##Diferenciacion estacional
Ddldian2=diff(dldian2,lag=12)###lag=s
```

```{r}
plot(dldian2)
plot(Ddldian2)
monthplot(Ddldian2)
acf(as.numeric(Ddldian2),lag.max = length(dian2)/4)
spectrum(Ddldian2)
nsdiffs(Ddldian2)
```
Notese que ya es posible observar una serie estacionaria luego de realizar la diferenciación ordinaria y la diferenciación estacionaria, además no parece ser necesario realizar más diferenciaciones. 
Ahora si procedemos a encontrar los ordenes para los modelos de la familia SARIMA.

```{r}
acf(as.numeric(Ddldian2))
acf(as.numeric(Ddldian2),lag.max = length(dian2)/4, ci.type='ma')# q=0,1,2 Q=0,1
pacf(as.numeric(Ddldian2),lag.max = length(dian2)/4)#p=0,1,2,...,5, P=0,1,2,3
```
Luego de analizar los gráficos del ACF y PACF se procede a ajustar el modelo SARIMA.
## Ajuste del modelo 
```{r,warning=FALSE}
##Ajuste del modelo SARIMA
modelo_SARIMA = Arima(df_train4, c(5, 1, 1),seasonal = list(order = c(3, 1, 2), period = 12),lambda = 0)
coeftest(modelo_SARIMA)
##Refinando el modelo SARIMA.
modelo_SARIMA_ref = Arima(df_train4, c(5, 1, 1),seasonal = list(order = c(3, 1, 2), period = 12),lambda = 0,fixed=c(0,NA,0,0,0,NA,NA,NA,NA,0,0))
coeftest(modelo_SARIMA_ref)
```
## Verificación de los supusetos.
Ahora procederemos a revisar los supusetos de este modelo.
```{r }
# Análisis de residuales
residuales=modelo_SARIMA_ref$residuals
plot(residuales)
acf(as.numeric(residuales))
acf(as.numeric(residuales^2))
pacf(as.numeric(residuales))
```
Notese que los acf y pacf parecen indicar que no queda nada por ser explicado en los residuales del modelo, lo cual es un buen indicio.

```{R}
#Test de normalidad
tseries::jarque.bera.test(residuales)
#Test de autocorrelación
length(residuales)/4
sqrt(length(residuales))
Box.test(residuales, lag =17 , type = "Ljung-Box", fitdf = 2)#No puedo Rechazar la hipótesis de no autocorrelación!
```
En este caso se tiene que se rechaza el supuesto de normalidad ya que 2.2e-16 (p-value) es menor que un valor alpha de 0.05,en este caso el p-valor de la prueba Box-Ljung es 0.9599, que es mayor que 0.05, por lo tanto hay suficiente evidencia para rechazar que hay autocorrelación significativa en los residuos.


```{R}
###Estad?sticas CUSUM
res=residuales
cum=cumsum(res)/sd(res)
N=length(res)
cumq=cumsum(res^2)/sum(res^2)
Af=0.948 ###Cuantil del 95% para la estad?stica cusum
co=0.10997####Valor del cuantil aproximado para cusumsq para n/2
LS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)
LI=-LS
LQS=co+(1:length(res))/N
LQI=-co+(1:length(res))/N
plot(cum,type="l",ylim=c(min(LI),max(LS)),xlab="t",ylab="",main="CUSUM")
lines(LS,type="S",col="red")
lines(LI,type="S",col="red")
#CUSUMSQ
plot(cumq,type="l",xlab="t",ylab="",main="CUSUMSQ")                      
lines(LQS,type="S",col="red")                                                                           
lines(LQI,type="S",col="red")
```
## Rolling modelo SARIMA
Luesto de observar los supuestos sobre los residuales del modelo SARIMA ajustado procedemos a observar la capacidad predictiva del modelo haciendo rolling sobre el conjunto de prueba y obteniendo su ECM.

```{r,warning=FALSE}
h=1
lserie=length(df_train4)##datos de entrenamiento
ntrain=length(df_train4)
ntrain
time(df_train4)
time(df_train4)[ntrain]###Me entrega la ultima fecha de la posición ntrain
train=window(ldian2,end=c(2018,8))
test=window(ldian2, start = time(df_train4)[split2] + 1/12)
#length(train)
ntest=length(df_test4)
ntest
fcmat=matrix(0,nrow=ntest,ncol=h)
for(i in 1:ntest)
{
  x=window(ldian2,end=time(ldian2)[ntrain]+(i-1)/12)
  #print(length(x))
  refit=Arima(x, model=modelo_SARIMA_ref)
  fcmat[i,]=as.numeric(forecast::forecast(refit,h=h)$mean)
}
fcmat_menos_reales<-exp(verval[,1])-exp(fcmat)
ECM=apply(fcmat_menos_reales^2,MARGIN = 2,mean,na.rm=TRUE)
RECM=sqrt(ECM) 
RECM # 4953741
```
Se obtuvo un valor de ECM de 4.827798e+12 (Billones) y un RECM de 
2197225 (Millones).
## Predicción usando el modelo SARIMA.
```{r}
verval_ts<-ts(exp(test),start=time(ldian2)[ntrain]+1/12,frequency=12)
fchstepahe_ts<-ts(exp(fcmat),start=time(ldian2)[ntrain]+1/12,frequency=12)

plot(verval_ts, col = "blue", ylab = "Impuestos", xlab = "Tiempo")
lines(fchstepahe_ts, col = "red")
legend("topright", legend = c("Reales", "Predicciones"), col = c("blue", "red"), lty = 1)
```
# Modelamiento de los outliers (modelo SARIMA).
```{r}
resiO= residuals(modelo_SARIMA_ref)
coefO= coefs2poly(modelo_SARIMA_ref)
outliersSARIMA= locate.outliers(resiO,coefO,cval=4.5)
outliersSARIMA
n=length(df_train4)
xregSARIMA = outliers.effects(outliersSARIMA,n )
xregSARIMA
```
Notese que se obtuvieron 26 outliers, lo cual nos puede indicar que el modelo ajustado no es muy bueno para este modelo, por dicha razón no seria lo mejor ajustar outliers a este modelo.
```{r}
modelo_SARIMA_ref_outliers = Arima(df_train4, c(5, 1, 1),seasonal = list(order = c(3, 1, 2), period = 12),lambda = 0,fixed=c(0,NA,0,0,0,NA,NA,NA,0,0,0,NA,NA,0,NA,NA,NA,NA,0,0,NA,0),xreg =xregSARIMA )
coeftest(modelo_SARIMA_ref_outliers)
```
```{r}
## Revisando que no hay más outliers
resiO2= residuals(modelo_SARIMA_ref_outliers)
coefO2= coefs2poly(modelo_SARIMA_ref_outliers)
outliersSARIMA2= locate.outliers(resiO2,coefO2,cval=4.5)
outliersSARIMA2
n=length(df_train4)
xregSARIMA2 = outliers.effects(outliersSARIMA2,n )
xregSARIMA2
##No hay más outliers
```
```{r}
total_outliers2<-cbind(xregSARIMA,xregSARIMA2)
modelo_SARIMA_ref_outliers2 = Arima(df_train4, c(5, 1, 1),seasonal = list(order = c(3, 1, 2), period = 12),lambda = 0,fixed=c(0,NA,0,0,0,NA,NA,NA,0,0,0,NA,0,0,NA,0,0,0,0,0,NA,0,NA,NA),xreg =total_outliers2 )
coeftest(modelo_SARIMA_ref_outliers2)
```


## Verificación de los supusetos.
Ahora procederemos a revisar los supusetos de este modelo.
```{r }
# Análisis de residuales
residuales=modelo_SARIMA_ref_outliers2$residuals
plot(residuales)
acf(as.numeric(residuales))
acf(as.numeric(residuales^2))
pacf(as.numeric(residuales))
```

```{R}
#Test de normalidad
tseries::jarque.bera.test(residuales)
#Test de autocorrelación
length(residuales)/4
sqrt(length(residuales))
Box.test(residuales, lag =17 , type = "Ljung-Box", fitdf = 2)#No puedo Rechazar la hipótesis de no autocorrelación!
```
Por el p value encontrado en el test de Jarque bera (1.059e-08<0.05) se tiene suficiente evidencia estadistica para rechazar la hipotesis de normalidad, es decir que lso residuales no siguen una distribución normal, por otro lado se tiene un p valor de 0.9216 en la prueba de Box-Ljung, lo cual nos indica que hay suficiente evidencia estadistica para rechazar la hipótesis de Autocorrelación dentro de los residuales.

```{R}
### Estadisticas CUSUM
res=residuales
cum=cumsum(res)/sd(res)
N=length(res)
cumq=cumsum(res^2)/sum(res^2)
Af=0.948 ###Cuantil del 95% para la estad?stica cusum
co=0.10997####Valor del cuantil aproximado para cusumsq para n/2
LS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)
LI=-LS
LQS=co+(1:length(res))/N
LQI=-co+(1:length(res))/N
plot(cum,type="l",ylim=c(min(LI),max(LS)),xlab="t",ylab="",main="CUSUM")
lines(LS,type="S",col="red")
lines(LI,type="S",col="red")
#CUSUMSQ
plot(cumq,type="l",xlab="t",ylab="",main="CUSUMSQ")                      
lines(LQS,type="S",col="red")                                                                           
lines(LQI,type="S",col="red")
```

## Rolling sobre el modelo SARIMA con outliers.
```{r}
h=1
num_outliers=dim(total_outliers2)[2]
regresoras_aditivos1=matrix(c(rep(0,h*(num_outliers-7))),h,num_outliers-7)
#regresoras_LS=matrix(c(rep(1,h)),h,1)
#regresoras_aditivos2=matrix(c(rep(0,h)),h,1)
regresoras_TC=matrix(c(rep(0,h)),h,7)
regresoras=cbind(regresoras_aditivos1,regresoras_TC)
colnames(regresoras)=colnames(total_outliers2)
```

```{r}
prediccSARIMAo <- matrix(0, nrow=ntest, ncol=h) 
verval <- cbind(test[1:ntest])
for(i in 1:(ntest)){
  x<-window(ldian2,end=time(ldian2)[ntrain]+(i-1)/12)
  refit <- Arima(x, model=modelo_SARIMA_ref_outliers2,xreg = total_outliers2)
  prediccSARIMAo[i,] <- forecast::forecast(refit, xreg=regresoras,h=h)$mean
  total_outliers2<-rbind(total_outliers2,regresoras)
}
errores_predSARIMAo <- exp(verval) -exp(prediccSARIMAo)
ECM_SARIMAo <- mean(errores_predSARIMAo^2) # Medida de precisión del pronóstico (ECM).
RECM_SARIMAo <- sqrt(ECM_SARIMAo) # Se le saca raíz 
RECM_SARIMAo
```
Se obtuvo un valor ECM de  4.827798e+12 y un valor RECM de 2129751.
## Prediccion utilizando el modelo SARIMA con outliers.
```{r}
verval_ts<-ts(exp(verval),start=time(ldian2)[ntrain]+1/12,frequency=12)
fchstepahe_ts<-ts(exp(prediccSARIMAo),start=time(ldian2)[ntrain]+1/12,frequency=12)

plot(verval_ts, col = "blue", ylab = "Impuestos", xlab = "Tiempo")
lines(fchstepahe_ts, col = "red")
legend("topright", legend = c("Reales", "Predicciones"), col = c("blue", "red"), lty = 1)
```

# Modelamiento de los outliers (modelo AR).
```{r}
resiO= residuals(ARPURO_ref_prueba)
coefO= coefs2poly(ARPURO_ref_prueba)
outliersSARIMA= locate.outliers(resiO,coefO,cval=4.5)
outliersSARIMA
n=length(df_train4)
xregAR = outliers.effects(outliersSARIMA,n )
xregAR
```

```{r}
ARPURO_ref_prueba_outliers=Arima(ElimiTenddian_STL_dummy,order=c(1,0,0),seasonal = c(1,0,0),include.mean = F,xreg=xregAR,fixed=c(NA,NA,NA,NA,NA,NA,0))
#coeftest(modelo_AR_ref_outliers)
```
```{r}
resiO= residuals( ARPURO_ref_prueba_outliers)
coefO= coefs2poly( ARPURO_ref_prueba_outliers)
outliersSARIMA= locate.outliers(resiO,coefO,cval=4.5)
outliersSARIMA
n=length(df_train4)
xregAR2 = outliers.effects(outliersSARIMA,n )
xregAR2
```
```{r}
total_outliers=cbind(xregAR,xregAR2)
 ARPURO_ref_prueba_outliers2=Arima(ElimiTenddian_STL_dummy,order=c(1,0,0),seasonal = c(1,0,0),include.mean = F,xreg=total_outliers,fixed=c(NA,NA,NA,NA,NA,NA,0,NA))
coeftest( ARPURO_ref_prueba_outliers2)
```

## Verificación de los supuestos modelo AR(12) con Outliers.
```{r }
# Análisis de residuales
residuales= ARPURO_ref_prueba_outliers2$residuals
plot(residuales)
acf(as.numeric(residuales))
acf(as.numeric(residuales^2))
pacf(as.numeric(residuales))
```
```{R}
#Test de normalidad
tseries::jarque.bera.test(residuales)
#Test de autocorrelación
length(residuales)/4
sqrt(length(residuales))
Box.test(residuales, lag =17 , type = "Ljung-Box", fitdf = 2)#No puedo Rechazar la hipótesis de no autocorrelación!
```
```{R}
###Estad?sticas CUSUM
res=residuales
cum=cumsum(res)/sd(res)
N=length(res)
cumq=cumsum(res^2)/sum(res^2)
Af=0.948 ###Cuantil del 95% para la estad?stica cusum
co=0.10997####Valor del cuantil aproximado para cusumsq para n/2
LS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)
LI=-LS
LQS=co+(1:length(res))/N
LQI=-co+(1:length(res))/N
plot(cum,type="l",ylim=c(min(LI),max(LS)),xlab="t",ylab="",main="CUSUM")
lines(LS,type="S",col="red")
lines(LI,type="S",col="red")
#CUSUMSQ
plot(cumq,type="l",xlab="t",ylab="",main="CUSUMSQ")                      
lines(LQS,type="S",col="red")                                                                           
lines(LQI,type="S",col="red")
```
## Rolling sobre el modelo AR con outliers.
```{r}
h=1
num_outliers=dim(total_outliers)[2]
regresoras_aditivos1=matrix(c(rep(0,h*(num_outliers-4))),h,num_outliers-4)
#regresoras_LS=matrix(c(rep(1,h)),h,1)
regresoras_aditivos2=matrix(c(rep(0,h)),h,1)
regresoras_TC=matrix(c(rep(0,h)),h,3)
regresoras=cbind(regresoras_aditivos1,regresoras_TC,regresoras_aditivos2)
colnames(regresoras)=colnames(total_outliers)
```

```{r}
prediccARo <- matrix(0, nrow=ntest, ncol=h) 
verval <- cbind(test[1:ntest])
for(i in 1:(ntest)){
  x <- window((ElimiTenddian_STL-dian_pred), end = time(ElimiTenddian_STL-dian_pred)[ntrain]+(i-1)/12)
  refit <- Arima(x, model=ARPURO_ref_prueba_outliers2,xreg = total_outliers)
  prediccARo[i,] <- forecast::forecast(refit, xreg=regresoras,h=h)$mean
  total_outliers<-rbind(total_outliers,regresoras)
}
prediccARo1<-(prediccARo+estacionalidad[226:282])+tendencia[226:282]
errores_predARo <- exp(verval) -exp(prediccARo1)
ECM_ARo <- mean(errores_predARo^2) # Medida de precisión del pronóstico (ECM).
RECM_ARo <- sqrt(ECM_ARo) # Se le saca raíz 
RECM_ARo
```
Notese que se obtuvo un valor ECM de 1.697165e+12 y un valor de RECM de 1302753.
## Prediccion utilizando el modelo AR con outliers
```{r}
verval_ts<-ts(exp(verval[,1]),start=time(ldian2)[ntrain]+1/12,frequency=12)
fchstepahe_ts<-ts(exp(prediccARo1),start=time(ldian2)[ntrain]+1/12,frequency=12)

plot(verval_ts, col = "blue", ylab = "Impuestos", xlab = "Tiempo")
lines(fchstepahe_ts, col = "red")
legend("topright", legend = c("Reales", "Predicciones"), col = c("blue", "red"), lty = 1)
```
